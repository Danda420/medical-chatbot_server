{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install transformers indobenchmark-toolkit bitsandbytes pyngrok flask flask-cors\n!pip install unsloth \"xformers==0.0.28.post2\"\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth @ git+https://github.com/unslothai/unsloth.git\"\n!pip uninstall -y torch torchvision torchaudio\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:32:36.954632Z","iopub.execute_input":"2024-11-29T09:32:36.955010Z","iopub.status.idle":"2024-11-29T09:35:29.699222Z","shell.execute_reply.started":"2024-11-29T09:32:36.954976Z","shell.execute_reply":"2024-11-29T09:35:29.698233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:35:29.701277Z","iopub.execute_input":"2024-11-29T09:35:29.701623Z","iopub.status.idle":"2024-11-29T09:35:29.706403Z","shell.execute_reply.started":"2024-11-29T09:35:29.701593Z","shell.execute_reply":"2024-11-29T09:35:29.705514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import Optional\nimport re\nimport os\nimport torch\nimport random\nimport numpy as np\nimport bitsandbytes\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom pyngrok import ngrok\nimport subprocess\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom transformers import MBartForConditionalGeneration, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\nfrom indobenchmark import IndoNLGTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:35:29.707490Z","iopub.execute_input":"2024-11-29T09:35:29.707857Z","iopub.status.idle":"2024-11-29T09:35:29.724369Z","shell.execute_reply.started":"2024-11-29T09:35:29.707818Z","shell.execute_reply":"2024-11-29T09:35:29.723694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\ndef format_text(text):\n    text = text.strip()\n    sentences = re.split('([.!?])', text)\n    sentences = [s.strip().capitalize() for s in sentences if s.strip()]\n    formatted_text = ''.join([f'{s} ' if s in '.!?' else s for s in sentences]).strip()\n    return formatted_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:35:29.726397Z","iopub.execute_input":"2024-11-29T09:35:29.726644Z","iopub.status.idle":"2024-11-29T09:35:29.737987Z","shell.execute_reply.started":"2024-11-29T09:35:29.726621Z","shell.execute_reply":"2024-11-29T09:35:29.737080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ngrok.set_auth_token(\"2pKBQen0CRf2LTplCUtNrNWacfi_4zEqemo7u5MtkMZef3PTY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:35:29.739098Z","iopub.execute_input":"2024-11-29T09:35:29.739386Z","iopub.status.idle":"2024-11-29T09:35:29.787487Z","shell.execute_reply.started":"2024-11-29T09:35:29.739359Z","shell.execute_reply":"2024-11-29T09:35:29.786843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load the pre-trained model and tokenizer\nmodel_name_bart = \"Danda245/BART-medical-qa-id\"\nmodel_name_gpt2 = \"lafarizo/indo_medical_gpt2_v4\"\nmodel_name_t5 = \"Yaziddd/indoT5-base-medical-qa-3\"\nmodel_name_phi = \"Rizald95/phi.35-instruct-medical-qa-id\"\n\nmodel_bart = MBartForConditionalGeneration.from_pretrained(model_name_bart).to(device)\ntokenizer_bart = IndoNLGTokenizer.from_pretrained(model_name_bart)\n\nmodel_gpt2 = AutoModelForCausalLM.from_pretrained(model_name_gpt2).to(device)\ntokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name_gpt2)\nif tokenizer_gpt2.pad_token is None:\n    tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n\nmodel_t5 = AutoModelForSeq2SeqLM.from_pretrained(model_name_t5).to(device)\ntokenizer_t5 = AutoTokenizer.from_pretrained(model_name_t5, use_fast=True)\n\nmodel_instance_phi, tokenizer_phi = FastLanguageModel.from_pretrained(model_name_phi)\nmodel_phi = FastLanguageModel.for_inference(model_instance_phi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:35:29.788396Z","iopub.execute_input":"2024-11-29T09:35:29.788643Z","iopub.status.idle":"2024-11-29T09:35:47.599658Z","shell.execute_reply.started":"2024-11-29T09:35:29.788619Z","shell.execute_reply":"2024-11-29T09:35:47.598632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enable CORS for Flask app\napp = Flask(__name__)\nCORS(app, resources={r\"/generate\": {\"origins\": \"*\"}})\n\n@app.route('/generate', methods=['POST'])\ndef generate():\n    user_input = request.json.get('input', '')\n    formatted_input = (\n        f\"Input:\\n {user_input}\\n\"\n        \"Response:\"\n    )\n\n    ########\n    # BART #\n    ########\n    def generate_bart():\n        set_seed(101)\n        inputs_bart = tokenizer_bart.prepare_input_for_generation(\n            user_input.lower(),\n            return_tensors='pt',\n            lang_token='[indonesian]',\n            decoder_lang_token='[indonesian]',\n        ).to(device)\n        \n        input_ids_bart = inputs_bart['input_ids'].to(device)\n        attention_mask_bart = inputs_bart['attention_mask'].to(device)\n\n        if input_ids_bart.dim() == 1:\n            input_ids_bart = input_ids_bart.unsqueeze(0)\n            attention_mask_bart = attention_mask_bart.unsqueeze(0)\n\n        with torch.no_grad():\n            outputs_bart = model_bart.generate(\n                input_ids=input_ids_bart,\n                attention_mask=attention_mask_bart,\n                max_length=128,\n                num_beams=5,\n                top_k=20,\n                top_p=0.7,\n                no_repeat_ngram_size=2,\n                do_sample=True,\n                temperature=0.7,\n            )\n\n        generated_ans_bart = tokenizer_bart.decode(\n            outputs_bart[0],\n            skip_special_tokens=True\n        )\n\n        if generated_ans_bart.startswith(user_input.lower()):\n            generated_ans_bart = generated_ans_bart[len(user_input):].strip()\n            \n        return 'BART', format_text(generated_ans_bart)\n\n    ########\n    # GPT2 #\n    ########\n    def generate_gpt2():\n        set_seed(11)\n        inputs_gpt2 = tokenizer_gpt2(\n            user_input, \n            return_tensors=\"pt\", \n            truncation=True, \n            padding=True,\n        ).to(device)\n        \n        input_ids_gpt2 = inputs_gpt2['input_ids'].to(device)\n        attention_mask_gpt2 = inputs_gpt2['attention_mask'].to(device)\n\n        with torch.no_grad():\n            outputs_gpt2 = model_gpt2.generate(\n                input_ids=input_ids_gpt2,\n                attention_mask=attention_mask_gpt2,\n                max_length=128,\n                num_beams=5,\n                top_k=20,\n                top_p=0.7,\n                no_repeat_ngram_size=2,\n                do_sample=True,\n                temperature=0.7,\n                eos_token_id=tokenizer_gpt2.eos_token_id,\n                pad_token_id=tokenizer_gpt2.pad_token_id\n            )\n\n        generated_ans_gpt2 = tokenizer_gpt2.decode(\n            outputs_gpt2[0], \n            skip_special_tokens=True\n        )\n\n        if generated_ans_gpt2.lower().startswith(user_input.lower()):\n            generated_ans_gpt2 = generated_ans_gpt2[len(user_input):].strip()\n\n        return 'GPT2', generated_ans_gpt2\n\n    ######\n    # T5 #\n    ######\n    def generate_t5():\n        set_seed(41)\n        inputs_t5 = tokenizer_t5(\n            user_input,\n            return_tensors=\"pt\",\n            truncation=True, \n            padding=True,\n        ).to(device)\n        \n        input_ids_t5 = inputs_t5['input_ids'].to(device)\n        attention_mask_t5 = inputs_t5['attention_mask'].to(device)\n\n        if input_ids_t5.dim() == 1:\n            input_ids_t5 = input_ids_t5.unsqueeze(0)\n            attention_mask_t5 = attention_mask_t5.unsqueeze(0)\n            \n        with torch.no_grad():\n            outputs_t5 = model_t5.generate(\n                input_ids=input_ids_t5,\n                attention_mask=attention_mask_t5,\n                max_length=128,\n                num_beams=5,\n                top_k=20,\n                top_p=0.7,\n                no_repeat_ngram_size=2,\n                do_sample=True,\n                temperature=0.7,\n            )\n\n        generated_ans_t5 = tokenizer_t5.decode(\n            outputs_t5[0], \n            skip_special_tokens=True\n        )\n\n        if generated_ans_t5.lower().startswith(user_input.lower()):\n            generated_ans_t5 = generated_ans_t5[len(user_input):].strip()\n            \n        return 'T5', generated_ans_t5\n\n    def generate_phi():\n        set_seed(3407)\n        inputs_phi = tokenizer_phi(\n            formatted_input, \n            return_tensors=\"pt\",\n            truncation=True, \n            padding=True,\n        ).to(device)\n\n        input_ids_phi = inputs_phi['input_ids'].to('cuda')\n        attention_mask_phi = inputs_phi['attention_mask'].to('cuda')\n\n        with torch.no_grad():\n            outputs_phi = model_phi.generate(\n                input_ids=input_ids_phi, \n                attention_mask=attention_mask_phi,\n                max_new_tokens=128,\n                num_beams=1,\n                top_k=20,\n                top_p=0.7,\n                no_repeat_ngram_size=2,\n                do_sample=True,\n                temperature=0.7,\n            )\n\n        generated_ans_phi = tokenizer_phi.decode(\n            outputs_phi[0], \n            skip_special_tokens=True\n        )\n\n        if generated_ans_phi.lower().startswith(formatted_input.lower()):\n            generated_ans_phi = generated_ans_phi[len(formatted_input):].strip()\n\n        return 'PHI3.5', generated_ans_phi\n\n    #####################################\n    # Run model generations in parallel #\n    #####################################\n    with ThreadPoolExecutor() as executor:\n        futures = {\n            executor.submit(generate_bart): 'BART',\n            executor.submit(generate_gpt2): 'GPT2',\n            executor.submit(generate_t5): 'T5',\n            executor.submit(generate_phi): 'PHI3.5',\n        }\n\n        results = {}\n        for future in as_completed(futures):\n            model_name, output = future.result()\n            results[model_name] = output\n\n    return jsonify(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:35:54.566591Z","iopub.execute_input":"2024-11-29T09:35:54.566875Z","iopub.status.idle":"2024-11-29T09:35:54.584313Z","shell.execute_reply.started":"2024-11-29T09:35:54.566847Z","shell.execute_reply":"2024-11-29T09:35:54.583407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    public_url = ngrok.connect(5000)\n    print(f\"Public URL: {public_url}\")\n    app.run(host='0.0.0.0', port=5000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:35:54.585306Z","iopub.execute_input":"2024-11-29T09:35:54.585550Z"}},"outputs":[],"execution_count":null}]}